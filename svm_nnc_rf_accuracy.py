# -*- coding: utf-8 -*-
"""8_18101127_SakibHassanChowdhury.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dQqMUfcGyHIidsqMhb-0o1MkIdaHibdj
"""

import pandas as pd
import numpy as np

income = pd.read_csv('/content/sample_data/Income Dataset (50k).csv')
income.head(3)

income.shape

#Checking how many null/missing values are there
income.isnull().sum()

#As seen from the above output, the missing values are all in the categorical features
#So, the missing values are filled with 'Unknown'
income = income.fillna('Unknown')
income.head(60)

#To check the data types of each column
income.info()

#Dropping the unnecessary and less important features(marital-staus, relationship)
#'education' is dropped as there is already a 'educational-num' feature 
#which can cover the necessary educational ranking as it has ordered numbers for each degree/qualification
#'native-country' is dropped as the 'race' feature already covers that part better
income = income.drop(['education', 'marital-status', 'relationship', 'native-country'], axis = 1)
income.head()

income.shape

income['workclass'].unique()

income['occupation'].unique()

income['race'].unique()

income['gender'].unique()

#One-hot encoding applied as the following categorical features are all nominal features
income = pd.get_dummies(income, columns=["workclass", "occupation", "race", "gender"], drop_first=True)

income.head()

income.shape

#Splitting the dataset into features and label
#Here, 'income_>50K' has been considered as a label
#So, X is the array of the features(data) and Y is the array of the label(target)
X = np.array(income.drop(['income_>50K'],1))
Y = np.array(income['income_>50K'])

X.shape

Y.shape

#Splitting the dataset into 8:2 training set and test set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1)
print(X_train.shape)
print(X_test.shape)

#Using MinMax scaler to scale all the values between 0-1
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

scaler.fit(X_train)

#transform train data
X_train_scaled = scaler.transform(X_train)

#MinMax values before scaling
print("per-feature minimum before scaling:\n {}".format(X_train.min(axis=0)))
print("per-feature maximum before scaling:\n {}".format(X_train.max(axis=0)))

#MinMax values after scaling
print("per-feature minimum after scaling:\n {}".format(X_train_scaled.min(axis=0)))
print("per-feature maximum after scaling:\n {}".format(X_train_scaled.max(axis=0)))

#transform test data
X_test_scaled = scaler.transform(X_test)

#Accuracy without scaling
from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier()

knn.fit(X_train, y_train)

print("Test set accuracy: {:.2f}".format(knn.score(X_test, y_test)))

#Improved accuracy after scaling

#train
knn.fit(X_train_scaled, y_train)

# scoring on the scaled test set
print("Scaled test set accuracy: {:.2f}".format(knn.score(X_test_scaled, y_test)))

#Applying Support Vector Classifier
from sklearn.svm import SVC
svc = SVC(kernel="linear")
svc.fit(X_train_scaled, y_train)

#Accuracy of SVM Model
score_svc = svc.score(X_test_scaled, y_test)
print("Testing accuracy of the model is {:.2f}".format(score_svc))

#Applying MLPClassifier(Neural Network)
from sklearn.neural_network import MLPClassifier
nnc=MLPClassifier(hidden_layer_sizes=(7), activation="relu", max_iter=10000)
nnc.fit(X_train_scaled, y_train)

#Accuracy of MLPClassifier(Neural Network) Model
score_nnc = nnc.score(X_test_scaled, y_test)
print("The Testing accuracy of the model is {:.2f}".format(score_nnc))

#Applying Ensemble Classifier(Random Forest)
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=50)
rfc.fit(X_train_scaled, y_train)

#Accuracy of Random Forest Model
score_rfc = rfc.score(X_test_scaled, y_test)
print("The Testing accuracy of the model is {:.2f}".format(score_rfc))

#Performing dimensionality reduction using PCA
from sklearn.decomposition import PCA 
#Reducing the number of feature vectors into half which in this case is 33/2 = 16.5 or 16
pca = PCA(n_components=16)
pc_train= pca.fit_transform(X_train_scaled)
pc_test= pca.fit_transform(X_test_scaled)
print(pc_train)

pca.explained_variance_ratio_

sum(pca.explained_variance_ratio_)

#Applying Support Vector Classifier again after PCA
svc.fit(pc_train, y_train)

#Accuracy of SVM Model after PCA
score_pca_svc = svc.score(pc_test, y_test)
print("Testing accuracy of the model after PCA is {:.2f}".format(score_pca_svc))

#Applying MLPClassifier(Neural Network) again after PCA
nnc.fit(pc_train, y_train)

#Accuracy of MLPClassifier(Neural Network) Model after PCA
score_pca_nnc = nnc.score(pc_test, y_test)
print("Testing accuracy of the model after PCA is {:.2f}".format(score_pca_nnc))

#Applying Ensemble Classifier(Random Forest) again after PCA
rfc.fit(pc_train, y_train)

#Accuracy of Random Forest Model after PCA
score_pca_rfc = rfc.score(pc_test, y_test)
print("Testing accuracy of the model after PCA is {:.2f}".format(score_pca_rfc))

import matplotlib.pyplot as plt
import seaborn as sns

#First, making dataframe of the accuracy scores of the models pre-PCA and post-PCA for comparison on bargraph
BR = pd.DataFrame([["Pre-PCA Accuracies", score_svc, score_nnc, score_rfc], ["Post-PCA Accuracies", score_pca_svc, score_pca_nnc, score_pca_rfc]], columns = ['Model', 'SVM', 'NeuralNetwork', 'RandomForest'])
#For unpivoting the DataFrame from wide format to long format
BR = pd.melt(frame = BR, id_vars = 'Model', var_name = 'Classifiers', value_name = 'Score')
# create a figure and axis
fig, ax = plt.subplots(figsize = (18, 9))
#Comparing the accuracies of the models pre-PCA and post-PCA and 
#plotting them against each other as a bargraph using matplotlib and seaborn
sns.barplot(ax = ax, data = BR, x = 'Model', y = 'Score', hue = 'Classifiers')

