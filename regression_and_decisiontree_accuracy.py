# -*- coding: utf-8 -*-
"""8_18101127_SakibHassanChowdhury.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dQqMUfcGyHIidsqMhb-0o1MkIdaHibdj
"""

import pandas as pd
import numpy as np

income = pd.read_csv('/content/sample_data/Income Dataset (50k).csv')
income.head(3)

income.shape

#Checking how many null/missing values are there
income.isnull().sum()

#As seen from the above output, the missing values are all in the categorical features
#So, the missing values are filled with 'Unknown'
income = income.fillna('Unknown')
income.head(60)

#To check the data types of each column
income.info()

#Dropping the unnecessary and less important features(marital-staus, relationship)
#'education' is dropped as there is already a 'educational-num' feature 
#which can cover the necessary educational ranking as it has ordered numbers for each degree/qualification
#'native-country' is dropped as the 'race' feature already covers that part better
income = income.drop(['education', 'marital-status', 'relationship', 'native-country'], axis = 1)
income.head()

income.shape

income['workclass'].unique()

income['occupation'].unique()

income['race'].unique()

income['gender'].unique()

#One-hot encoding applied as the following categorical features are all nominal features
income = pd.get_dummies(income, columns=["workclass", "occupation", "race", "gender"], drop_first=True)

income.head()

income.shape

#Splitting the dataset into features and label
#Here, 'income_>50K' has been considered as a label
#So, X is the array of the features(data) and Y is the array of the label(target)
X = np.array(income.drop(['income_>50K'],1))
Y = np.array(income['income_>50K'])

X.shape

Y.shape

#Splitting the dataset into training set and test set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1)
print(X_train.shape)
print(X_test.shape)

#Using MinMax scaler to scale all the values between 0-1
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

scaler.fit(X_train)

#transform train data
X_train_scaled = scaler.transform(X_train)

#MinMax values before scaling
print("per-feature minimum before scaling:\n {}".format(X_train.min(axis=0)))
print("per-feature maximum before scaling:\n {}".format(X_train.max(axis=0)))

#MinMax values after scaling
print("per-feature minimum after scaling:\n {}".format(X_train_scaled.min(axis=0)))
print("per-feature maximum after scaling:\n {}".format(X_train_scaled.max(axis=0)))

#transform test data
X_test_scaled = scaler.transform(X_test)

#Accuracy without scaling
from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier()

knn.fit(X_train, y_train)

print("Test set accuracy: {:.2f}".format(knn.score(X_test, y_test)))

#Improved accuracy after scaling

#train
knn.fit(X_train_scaled, y_train)

# scoring on the scaled test set
print("Scaled test set accuracy: {:.2f}".format(knn.score(X_test_scaled, y_test)))

#Import the dependencies for logistic regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

#Perform the classification and calculate accuracy using Logistic Regression
LRclassifier = LogisticRegression()
LRclassifier.fit(X_train_scaled, y_train) #Training the model
predictions = LRclassifier.predict(X_test_scaled)
print(predictions)# printing predictions

#Accuracy score of logistic regression
LRscore = accuracy_score(y_test, predictions)
print( LRscore )

#Import module for decision tree
from sklearn.tree import DecisionTreeClassifier

#Perform the classification and calculate accuracy using Decision Tree
DTclassifier = DecisionTreeClassifier(criterion='entropy',random_state=1)
DTclassifier.fit(X_train_scaled,y_train)
y_pred = DTclassifier.predict(X_test_scaled)
#Accuracy score of decision tree
DTscore = accuracy_score(y_pred,y_test)
print(DTscore)

import matplotlib.pyplot as plt
import seaborn as sns

#First, making dataframe of the accuracy scores of the models for comparison on barchart
BR = pd.DataFrame([["Logistic Regression", LRscore], ["Decision Tree", DTscore]], columns = ['Model', 'Accuracy'])
#For unpivoting the DataFrame from wide format to long format
BR = pd.melt(frame = BR, id_vars = 'Model', var_name = None, value_name = 'Score')
# create a figure and axis
fig, ax = plt.subplots()
#Comparing the accuracy and plotting them as a bar chart using matplotlib and seaborn
sns.barplot(ax = ax, data = BR, x = 'Model', y = 'Score')

